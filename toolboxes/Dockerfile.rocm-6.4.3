FROM fedora:rawhide

# getting Rocm6.4.3 repos
RUN <<EOF
tee /etc/yum.repos.d/rocm.repo <<REPO
[ROCm-6.4.3]
name=ROCm6.4.3
baseurl=https://repo.radeon.com/rocm/el9/6.4.3/main
enabled=1
priority=50
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
REPO

EOF

# Install build dependencies and tools
RUN dnf install -y \
       make gcc cmake lld clang clang-devel compiler-rt libcurl-devel \
       rocm rocminfo radeontop \
       git vim \
    && dnf clean all

# Set up working directory
WORKDIR /opt/llama.cpp

# Clone llama.cpp repository (with submodules)
RUN git clone --recursive https://github.com/ggerganov/llama.cpp.git .

# Build llama.cpp with HIP support
RUN git clean -xdf \
    && git pull \
    && git submodule update --recursive \
    && \
    # Configure and compile with HIP toolchain
    HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
      cmake -S . -B build \
            -DGGML_HIP=ON \
            -DAMDGPU_TARGETS=gfx1151 \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_HIP_UMA=ON \
    && cmake --build build --config Release -- -j$(nproc) \
    && cmake --install build --config Release

RUN find /opt/llama.cpp/build -type f -name 'lib*.so*' -exec cp {} /usr/lib64/ \; \
 && ldconfig

RUN printf '%s\n' \
  'export ROCBLAS_USE_HIPBLASLT=1' \
  > /etc/profile.d/rocm.sh \
  && chmod +x /etc/profile.d/rocm.sh \
  && echo 'source /etc/profile.d/rocm.sh' >> /etc/bashrc

COPY gguf-vram-estimator.py /usr/local/bin/gguf-vram-estimator.py
RUN chmod +x /usr/local/bin/gguf-vram-estimator.py

# Default to interactive shell
CMD ["/bin/bash"]
